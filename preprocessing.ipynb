{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(object):\n",
    "\n",
    "    #stemmer = nltk.stem.RSLPStemmer()\n",
    "    tokenizer = TweetTokenizer(reduce_len=True, preserve_case=False)\n",
    "    special_char = ['$', '%', '&', '*', '(', ')', '_', '-', '+', '=', '{', '[', '}', ']', '~', '.', ',', ';', 'º', 'ª', '°', '¹', '²', '³']\n",
    "\n",
    "    # UniLex: Método Léxico para Análise de Sentimentos Textuais sobre Conteúdo de Tweets em Português Brasileiro*\n",
    "    stoplist_uniLex = ['a', 'agora', 'ainda', 'alguem', 'algum', 'alguma', 'algumas', 'alguns', 'ampla', 'amplas', 'amplo', 'amplos',\n",
    "     'ante', 'antes', 'ao', 'aos', 'apos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'ate', 'atraves',\n",
    "     'cada', 'coisa', 'coisas', 'com', 'como', 'contra', 'contudo', 'da', 'daquele', 'daqueles', 'das', 'de', 'dela',\n",
    "     'delas', 'dele', 'deles', 'depois', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'deste',\n",
    "     'destes', 'deve', 'devem', 'devendo', 'dever', 'devera', 'deverao', 'deveria', 'deveriam', 'devia', 'deviam',\n",
    "     'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em',\n",
    "     'enquanto', 'entre', 'era', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estao', 'estas', 'estava',\n",
    "     'estavam', 'estavamos', 'este', 'estes', 'estou', 'eu', 'fazendo', 'fazer', 'feita', 'feitas', 'feito', 'feitos',\n",
    "     'foi', 'for', 'foram', 'fosse', 'fossem', 'grande', 'grandes', 'ha', 'isso', 'isto', 'ja', 'la', 'lhe', 'lhes',\n",
    "     'lo', 'mas', 'me', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'muita', 'muitas',\n",
    "     'muito', 'muitos', 'na', 'nao', 'nas', 'nem', 'nenhum', 'nessa', 'nessas', 'nesta', 'nestas', 'ninguem', 'no',\n",
    "     'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'nunca', 'o', 'os', 'ou', 'outra', 'outras', 'outro',\n",
    "     'outros', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante',\n",
    "     'pode', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'pois', 'por', 'porem', 'porque', 'posso',\n",
    "     'pouca', 'poucas', 'pouco', 'poucos', 'primeiro', 'primeiros', 'propria', 'proprias', 'proprio', 'proprios',\n",
    "     'quais', 'qual', 'quando', 'quanto', 'quantos', 'que', 'quem', 'sao', 'se', 'seja', 'sejam', 'sem', 'sempre',\n",
    "     'sendo', 'sera', 'serao', 'seu', 'seus', 'si', 'sido', 'so', 'sob', 'sobre', 'sua', 'suas', 'talvez', 'tambem',\n",
    "     'tampouco', 'te', 'tem', 'tendo', 'tenha', 'ter', 'teu', 'teus', 'ti', 'tido', 'tinha', 'tinham', 'toda', 'todas',\n",
    "     'todavia', 'todo', 'todos', 'tu', 'tua', 'tuas', 'tudo', 'ultima', 'ultimas', 'ultimo', 'ultimos', 'um', 'uma',\n",
    "     'umas', 'uns', 'vendo', 'ver', 'vez', 'vindo', 'vir', 'vos', 'vos']\n",
    "\n",
    "    # Stopwords do nltk + stopwords do UniLex\n",
    "    stoplist = sorted(set(stoplist_uniLex + stopwords.words('portuguese')))\n",
    "    #stoplist = stopwords.words('portuguese')\n",
    "    \n",
    "    def process(self, tweet):\n",
    "        tweet = self.to_lower(tweet)\n",
    "        tweet = self.remove_links(tweet)\n",
    "        tweet = self.remove_mentions(tweet)\n",
    "        tweet = self.remove_hashtags(tweet)\n",
    "        tweet = self.remove_numbers(tweet)\n",
    "        tweet = self.replace_three_or_more(tweet)\n",
    "\n",
    "        palavras = self.tokenizer.tokenize(tweet)\n",
    "        palavras = self.remove_punctuation(palavras)\n",
    "        #palavras = self.remove_stopwords(palavras)\n",
    "        #palavras = self.applystemmer(palavras)\n",
    "\n",
    "        \n",
    "        #comentar palavras\n",
    "        palavras_processadas = []\n",
    "        for palavra in palavras:\n",
    "            if len(palavra) <= 3:\n",
    "                palavra = re.sub('[:;=8][\\-=^*\\']?[)\\]Dpb}]|[cCqd{(\\[][\\-=^*\\']?[:;=8]', 'bom', palavra)\n",
    "                palavra = re.sub('[:;=8][\\-=^*\\']?[(\\[<{cC]|[D>)\\]}][\\-=^*\\']?[:;=8]', 'ruim', palavra)\n",
    "                \n",
    "            if len(palavra) <= 2:\n",
    "                palavra = ''\n",
    "\n",
    "            for s in self.special_char:\n",
    "                palavra = palavra.replace(s, '')\n",
    "\n",
    "            palavras_processadas.append(palavra)\n",
    "\n",
    "        tweet = ' '.join(palavras_processadas)\n",
    "        tweet = self.remove_duplicated_spaces(tweet)\n",
    "        return tweet\n",
    "    \n",
    "    def to_lower(self, tweet):\n",
    "        return tweet.lower()\n",
    "\n",
    "    def remove_links(self, text):   \n",
    "        return re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    def remove_mentions(self, tweet):\n",
    "        return re.sub(\"@\\S+\", \"\", tweet)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        entity_prefixes = ['@','#']\n",
    "        for separator in  punctuation:\n",
    "            if separator not in entity_prefixes:\n",
    "                text = text.replace(separator,' ')\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            word = word.strip()\n",
    "            if word:\n",
    "                if word[0] not in entity_prefixes:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def remove_numbers(self, tweet):\n",
    "        return re.sub(\"\\d+\", \"\", tweet)\n",
    "\n",
    "    def replace_three_or_more(self, tweet):\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "        return pattern.sub(r\"\\1\\1\", tweet)\n",
    "\n",
    "    def remove_duplicated_spaces(self, tweet):\n",
    "        tweet = tweet.strip()\n",
    "        return re.sub(\" +\", \" \", tweet)\n",
    "\n",
    "    def remove_stopwords(self, palavras):\n",
    "        return [palavra for palavra in palavras if palavra not in self.stoplist]\n",
    "\n",
    "    def remove_punctuation(self, palavras):\n",
    "        return [palavra for palavra in palavras if palavra not in list(punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweets.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['preprocessed'] = data['preprocessed'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   id            1000 non-null   int64  \n",
      " 1   text          1000 non-null   object \n",
      " 2   preprocessed  1000 non-null   object \n",
      " 3   sentiment     0 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in enumerate(data['text']):\n",
    "    data.at[index, 'preprocessed'] =  preprocessor.process(str(data.at[index, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                não vai com nenhum gente numa pandemia\n",
       "1     governo libera logo essa acordo furado espera ...\n",
       "2     viagem nunca dar certo meio pandemia queria festa\n",
       "3     até porque hospitais estão vazios pandemia con...\n",
       "4     also foi bem antes pandemia acho tweet referia...\n",
       "5     reflita pobres miseráveis invisíveis muitos es...\n",
       "6     sou pessoa mais feliz mundo por ter acompanhad...\n",
       "7     naoo mais menos mes pandemia surtei voltei com...\n",
       "8     bolsonarismo disposto derrubar bolsonaro fazer...\n",
       "9     tudo der certo segunda dose braço pandemia não...\n",
       "10    discurso anti sus enfraqueceu pandemia sobrara...\n",
       "11                      rodada pandemia vai ter acabado\n",
       "12    podemos descarta aderir movimento impeachment ...\n",
       "13    presa meu antes depois início pandemia pra per...\n",
       "14    tava muito feia então sarrafo pandemia não tão...\n",
       "15      previsão pandemia vai acabar rodada brasileirão\n",
       "16    minha irmã outra mãe thread gente hoje vou exp...\n",
       "17    para não dizer que não falei das pérolas fique...\n",
       "18          vcs realmente não querem que pandemia acabe\n",
       "19                    achava que pandemia tava acabando\n",
       "20    então justamente pela dificuldade pandemia gte...\n",
       "21    lembrando aqui vez que passei dia todinho pand...\n",
       "22    sim ultima unica vez que algo tipo foi início ...\n",
       "23    pare sou uma pessoa mudada agr pandemia transf...\n",
       "24    trabalha pela torcida faz uma enquete sobre op...\n",
       "25    juiz negacionista que boa forma tentarem humil...\n",
       "26    correto seria condições iguais mas nada estão ...\n",
       "27    sim vou ser veia chata vai ficar falando minut...\n",
       "28    compartilhe dêlike dicas para realizar reuniõe...\n",
       "29    passei ano sem pra faculdade por causa pandemi...\n",
       "30    economia vai mal devemos aos governadores que ...\n",
       "31    ninguém vai sair pandemia mesmo momento mesma ...\n",
       "32    minha mãe prometeu que ano seguinte pra bienal...\n",
       "33    minha mãe votou bolsonaro desembarcou inicio p...\n",
       "34    teve jogo aqui não alterou nada índices pandem...\n",
       "35    dou por declarado fim pandemia estarei enviand...\n",
       "36    dos maiores absurdos que durante toda pandemia...\n",
       "37    hoje vemos brasileiros tem razão estarem desan...\n",
       "38    sempre junto isso com não gostar comemorar ano...\n",
       "39    antes não escutava podcast desde pré pandemia ...\n",
       "40    finanças não juridico admitiu que fizeram caga...\n",
       "41                                   sonho pós pandemia\n",
       "42    ninguém tem moral não existe isonomia pandemia...\n",
       "43    augusto jornalismo sujo anti democráticos pren...\n",
       "44                   pensou essa cena meio uma pandemia\n",
       "45    pandemia fez minha timidez voltar mal consigo ...\n",
       "46                                  engano foi pandemia\n",
       "47    meu país sério presidente que não sério neste ...\n",
       "48    muita falta respeito com torcedor que comprou ...\n",
       "49    pandemia causa impactos alfabetização crianças...\n",
       "Name: preprocessed, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['preprocessed'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'tweets_processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
